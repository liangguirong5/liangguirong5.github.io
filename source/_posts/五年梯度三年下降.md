---
title: 五年梯度三年下降
tags:
  - DeepLearning
categories: DeepLearning
mathjax: true
date: 2017-02-20 20:29:52
---

梯度下降，是当下最常见的神经网络参数调优算法。当我们需要最小化目标函数$J(\theta)$时，只需要沿着目标函数梯度的$\nabla_\theta J(\theta)$反方向去更新$\theta$，就能让目标函数越来越小。如果$J(\theta)$是凸函数（二次导数非负），那最后一定能收敛到全局最小值，否则可能收敛到局部最小值。

<!-- more -->

梯度下降的原理，学过二次函数曲线的高中生就看懂，不过也可以用泰勒展开加以证明：
$$
J(\theta+\Delta\theta) \approx J(\theta)+\nabla_\theta J(\theta)\Delta\theta
$$
我们要求每次更新$J$都要减小，所以必然有$J(\theta+\Delta\theta) - J(\theta)<0$，进而要求展式的一阶项恒小于零，可选择令
$$
\Delta\theta = -\alpha\nabla_\theta J(\theta)
$$
其中，步长$\alpha$是一个小的正常数，这就是梯度下降法。

如果把上面的泰勒展式写到二阶项：
$$
J(\theta+\Delta\theta) \approx J(\theta)+\nabla_\theta J(\theta)\Delta\theta+\nabla_{\theta}^2 J(\theta)\Delta\theta^2
$$
就可以推导出牛顿法，步长更新是一阶导与二阶导的比值的相反数。

# 三类梯度下降

三种梯度下降区别在于：选择用多少数据来计算目标函数的梯度。

## Batch gradient descent

又称Vanilla gradient descend，根据整个训练样本集来更新参数$\theta$,
$$
θ=θ−η⋅∇θJ(θ)
$$
因为我们每次更新都需要整个训练数据集参与计算，所以速度很慢，如果训练样本很多以至于不能放在内存，这种方法便无计可施了。此外，batch梯度法不能在线更新权值。

```python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

## Stochastic gradient descent

SGD即为随机梯度下降，每次权值更新只用到一个训练样本：
$$
θ=θ−η⋅∇θJ(θ;x(i);y(i))
$$
相比于batch方法，我们的SGD小朋友每次只用一个样本，及时更新权重，因此收敛的会更快，可以用于在线学习。SGD在训练中会表现出周期性的波动，如下图所示。

![](https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png)

这种抖动一方面使得训练中可以跳出局部最小，从而进入更好的位置进行梯度下降；另一方面可能造成收敛的难度加大，因为SGD总是容易冲过头。

但是如果慢慢的减小学习率的话，SGD还是可以同batch一样的收敛的。

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

## Mini-batch gradient descent

随机多样本梯度下降，集合了上面两种方法的特点，更新公式如下：
$$
θ=θ−η⋅∇θJ(θ;x^{(i:i+n)};y^{(i:i+n)})
$$
这种更新方式，不像SGD一样每次更新的幅度很大，从而更易于收敛。常见的batchsize是50~256。

```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

## 存在的问题

学习率很难选取，太大难以收敛，太小收敛慢；变学习率的变化规则也难以适应所有数据集；此外，所有的特征不一定需要同一个学习率；最后，马鞍面产生的0梯度问题，SGD也没法解决。

# 改进算法

